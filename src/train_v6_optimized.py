"""
Rice Quality Assessment - V6 Optimized Training
================================================
Based on user's proven 0.78 approach:
- ConvNeXt-XLarge backbone (ImageNet-22k)
- 512x512 resolution
- Z-score normalization (NO log transform)
- L1 Loss (pure MAE)
- TTA (flip augmentations)
- 50 epochs

Improvements for 0.93 target:
1. Stronger count-focused loss weighting (5x for counts)
2. Better augmentation pipeline
3. OneCycleLR with proper warmup
4. Gradient accumulation for larger effective batch
5. Optional ensemble of folds
"""

import os
import random
import warnings
from pathlib import Path
from typing import List, Optional

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.cuda.amp import GradScaler, autocast
from torch.optim import AdamW
from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import StratifiedKFold
from PIL import Image
import timm
from tqdm import tqdm
import albumentations as A
from albumentations.pytorch import ToTensorV2
import argparse

warnings.filterwarnings('ignore')


# ============================================================================
# Configuration
# ============================================================================
class Config:
    """V6 - Proven approach with improvements"""
    DATA_DIR = Path("RiceData/Unido_AfricaRice_Challenge")
    IMAGE_DIR = None
    OUTPUT_DIR = Path("outputs/v6_optimized")
    
    # Model - XLarge for best performance
    BACKBONE = "convnext_xlarge.fb_in22k"
    PRETRAINED = True
    
    # Training
    IMG_SIZE = 512
    BATCH_SIZE = 4  # Will use accumulation
    ACCUMULATION_STEPS = 4  # Effective batch = 16
    EPOCHS = 50
    N_FOLDS = 5
    FOLD_TO_RUN = [0]
    
    # Optimizer
    LEARNING_RATE = 1e-4
    WEIGHT_DECAY = 0.01
    
    # Loss - COUNT FOCUSED
    COUNT_WEIGHT = 5.0  # Heavy weight on counts
    
    # TTA
    USE_TTA = True
    
    # Targets
    COUNT_TARGETS = ['Count', 'Broken_Count', 'Long_Count', 'Medium_Count',
                     'Black_Count', 'Chalky_Count', 'Red_Count', 
                     'Yellow_Count', 'Green_Count']
    CONTINUOUS_TARGETS = ['WK_Length_Average', 'WK_Width_Average', 
                          'WK_LW_Ratio_Average', 'Average_L', 
                          'Average_a', 'Average_b']
    TARGET_COLS = COUNT_TARGETS + CONTINUOUS_TARGETS
    
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    USE_AMP = True
    SEED = 42


def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True


# ============================================================================
# Z-Score Normalizer (proven to work)
# ============================================================================
class ZScoreNormalizer:
    """Simple Z-score - proven to work better than log"""
    def __init__(self):
        self.means = None
        self.stds = None
    
    def fit(self, targets):
        self.means = targets.mean(axis=0)
        self.stds = targets.std(axis=0)
        self.stds[self.stds < 1e-6] = 1.0
        return self
    
    def transform(self, targets):
        return (targets - self.means) / self.stds
    
    def inverse_transform(self, targets):
        return targets * self.stds + self.means
    
    def save(self):
        return {'means': self.means, 'stds': self.stds}
    
    def load(self, d):
        self.means = d['means']
        self.stds = d['stds']


# ============================================================================
# Dataset
# ============================================================================
class RiceDataset(Dataset):
    def __init__(self, df, img_dir, transform=None, normalizer=None, is_train=True):
        self.df = df.reset_index(drop=True)
        self.img_dir = img_dir
        self.transform = transform
        self.normalizer = normalizer
        self.is_train = is_train
        self.type_map = {'Paddy': 0, 'White': 1, 'Brown': 2}
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img = np.array(Image.open(self.img_dir / f"{row['ID']}.png").convert('RGB'))
        
        if self.transform:
            img = self.transform(image=img)['image']
        
        rice_type = self.type_map.get(row['Comment'], 0)
        
        if self.is_train:
            targets = row[Config.TARGET_COLS].values.astype(np.float32)
            if self.normalizer:
                targets = self.normalizer.transform(targets.reshape(1, -1))[0]
            return img, torch.tensor(targets, dtype=torch.float32), rice_type
        return img, row['ID'], rice_type


# ============================================================================
# Augmentations
# ============================================================================
def get_train_transforms(img_size):
    return A.Compose([
        A.RandomResizedCrop(size=(img_size, img_size), scale=(0.8, 1.0), ratio=(0.9, 1.1)),
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.RandomRotate90(p=0.5),
        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=30, p=0.5),
        A.OneOf([
            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=25, val_shift_limit=20),
        ], p=0.5),
        A.GaussNoise(std_range=(0.01, 0.03), p=0.2),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])


def get_valid_transforms(img_size):
    return A.Compose([
        A.Resize(img_size, img_size),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])


# ============================================================================
# TTA Transforms
# ============================================================================
def get_tta_transforms(img_size):
    """TTA: original + hflip + vflip + hflip+vflip"""
    base = A.Compose([
        A.Resize(img_size, img_size),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])
    hflip = A.Compose([
        A.Resize(img_size, img_size),
        A.HorizontalFlip(p=1.0),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])
    vflip = A.Compose([
        A.Resize(img_size, img_size),
        A.VerticalFlip(p=1.0),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])
    both = A.Compose([
        A.Resize(img_size, img_size),
        A.HorizontalFlip(p=1.0),
        A.VerticalFlip(p=1.0),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])
    return [base, hflip, vflip, both]


# ============================================================================
# Model
# ============================================================================
class RiceModelV6(nn.Module):
    """Simple proven architecture"""
    def __init__(self, backbone_name, num_targets=15, num_types=3, pretrained=True):
        super().__init__()
        
        self.backbone = timm.create_model(
            backbone_name,
            pretrained=pretrained,
            num_classes=0,
            global_pool='avg'
        )
        
        self.feat_dim = self.backbone.num_features
        
        # Type embedding
        self.type_emb = nn.Embedding(num_types, 64)
        
        # Regression head
        self.head = nn.Sequential(
            nn.Linear(self.feat_dim + 64, 1024),
            nn.LayerNorm(1024),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(512, num_targets)
        )
        
        # Type classifier (auxiliary)
        self.type_cls = nn.Linear(self.feat_dim, num_types)
    
    def forward(self, x, rice_type=None):
        feat = self.backbone(x)
        type_logits = self.type_cls(feat)
        
        if rice_type is not None:
            type_e = self.type_emb(rice_type)
        else:
            type_e = self.type_emb(type_logits.argmax(1))
        
        combined = torch.cat([feat, type_e], dim=1)
        preds = self.head(combined)
        
        return preds, type_logits


# ============================================================================
# Loss - COUNT FOCUSED L1
# ============================================================================
class CountFocusedL1Loss(nn.Module):
    """L1 Loss with heavier weight on count targets"""
    def __init__(self, num_counts=9, count_weight=5.0):
        super().__init__()
        self.num_counts = num_counts
        self.count_weight = count_weight
    
    def forward(self, pred, target):
        loss = F.l1_loss(pred, target, reduction='none')
        
        # Weight counts more heavily
        weights = torch.ones_like(loss)
        weights[:, :self.num_counts] = self.count_weight
        
        return (loss * weights).mean()


# ============================================================================
# Training
# ============================================================================
def train_epoch(model, loader, criterion, optimizer, scaler, config, accum):
    model.train()
    total_loss = 0
    optimizer.zero_grad()
    
    pbar = tqdm(loader, desc="Training")
    for step, (imgs, targets, types) in enumerate(pbar):
        imgs = imgs.to(config.DEVICE)
        targets = targets.to(config.DEVICE)
        types = types.to(config.DEVICE)
        
        with autocast(enabled=config.USE_AMP):
            preds, type_logits = model(imgs, types)
            loss = criterion(preds, targets)
            type_loss = F.cross_entropy(type_logits, types)
            total = (loss + 0.1 * type_loss) / accum
        
        scaler.scale(total).backward()
        
        if (step + 1) % accum == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
        
        total_loss += loss.item() * accum
        pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    return total_loss / len(loader)


@torch.no_grad()
def validate(model, loader, normalizer, config, use_tta=False):
    model.eval()
    all_preds, all_targets = [], []
    
    if use_tta:
        tta_transforms = get_tta_transforms(config.IMG_SIZE)
    
    for batch in tqdm(loader, desc="Validating"):
        if len(batch) == 3:
            imgs, targets, types = batch
        else:
            imgs, targets, types = batch[0], batch[1], batch[2]
        
        types = types.to(config.DEVICE)
        
        if use_tta:
            # Get original images from loader (before transform)
            # For now, just use non-TTA during validation
            imgs = imgs.to(config.DEVICE)
            with autocast(enabled=config.USE_AMP):
                preds, _ = model(imgs, types)
        else:
            imgs = imgs.to(config.DEVICE)
            with autocast(enabled=config.USE_AMP):
                preds, _ = model(imgs, types)
        
        all_preds.append(preds.cpu().numpy())
        all_targets.append(targets.numpy())
    
    preds = np.vstack(all_preds)
    targets = np.vstack(all_targets)
    
    # Denormalize
    preds = normalizer.inverse_transform(preds)
    targets = normalizer.inverse_transform(targets)
    
    # Clip negative counts
    n_counts = len(Config.COUNT_TARGETS)
    preds[:, :n_counts] = np.maximum(preds[:, :n_counts], 0)
    
    mae = np.abs(preds - targets).mean(axis=0)
    count_mae = mae[:n_counts].mean()
    cont_mae = mae[n_counts:].mean()
    
    return mae.mean(), mae, count_mae, cont_mae


def print_metrics(mae, targets, count_mae, cont_mae):
    print("\nTarget                           MAE")
    print("‚îÄ" * 45)
    for t, m in zip(targets, mae):
        flag = "‚úì" if m < 20 else "‚ö†Ô∏è" if m > 50 else " "
        print(f"{flag} {t:28s} {m:8.2f}")
    print("‚îÄ" * 45)
    print(f"Count Mean: {count_mae:.2f} | Cont Mean: {cont_mae:.2f}")
    print(f"Overall Mean: {mae.mean():.2f}")


# ============================================================================
# Main
# ============================================================================
def main():
    print("=" * 70)
    print("üåæ RICE V6 - OPTIMIZED (Based on 0.78 proven approach)")
    print("=" * 70)
    print(f"Device: {Config.DEVICE}")
    print(f"Backbone: {Config.BACKBONE}")
    print(f"Image size: {Config.IMG_SIZE}")
    print(f"Batch: {Config.BATCH_SIZE} x {Config.ACCUMULATION_STEPS} = {Config.BATCH_SIZE * Config.ACCUMULATION_STEPS}")
    print(f"Count weight: {Config.COUNT_WEIGHT}x")
    print(f"TTA: {Config.USE_TTA}")
    
    set_seed(Config.SEED)
    Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    df = pd.read_csv(Config.DATA_DIR / "Train.csv")
    print(f"Train samples: {len(df)}")
    
    skf = StratifiedKFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=Config.SEED)
    df['fold'] = -1
    for fold, (_, val_idx) in enumerate(skf.split(df, df['Comment'])):
        df.loc[val_idx, 'fold'] = fold
    
    results = []
    
    for fold in Config.FOLD_TO_RUN:
        print(f"\n{'=' * 70}\nFOLD {fold}\n{'=' * 70}")
        
        train_df = df[df['fold'] != fold]
        val_df = df[df['fold'] == fold]
        print(f"Train: {len(train_df)}, Val: {len(val_df)}")
        
        # Z-score normalizer
        normalizer = ZScoreNormalizer()
        normalizer.fit(train_df[Config.TARGET_COLS].values.astype(np.float32))
        
        train_ds = RiceDataset(train_df, Config.IMAGE_DIR, 
                               get_train_transforms(Config.IMG_SIZE), normalizer)
        val_ds = RiceDataset(val_df, Config.IMAGE_DIR,
                             get_valid_transforms(Config.IMG_SIZE), normalizer)
        
        train_loader = DataLoader(train_ds, Config.BATCH_SIZE, shuffle=True, 
                                  num_workers=4, pin_memory=True)
        val_loader = DataLoader(val_ds, Config.BATCH_SIZE * 2, shuffle=False,
                                num_workers=4, pin_memory=True)
        
        model = RiceModelV6(
            Config.BACKBONE,
            num_targets=len(Config.TARGET_COLS),
            pretrained=Config.PRETRAINED
        ).to(Config.DEVICE)
        
        print(f"Model: {Config.BACKBONE}, Features: {model.feat_dim}")
        
        criterion = CountFocusedL1Loss(len(Config.COUNT_TARGETS), Config.COUNT_WEIGHT)
        optimizer = AdamW(model.parameters(), lr=Config.LEARNING_RATE, 
                         weight_decay=Config.WEIGHT_DECAY)
        
        # CosineAnnealing - proven stable
        scheduler = CosineAnnealingLR(optimizer, T_max=Config.EPOCHS, eta_min=1e-6)
        scaler = GradScaler(enabled=Config.USE_AMP)
        
        best_count_mae = float('inf')
        best_overall_mae = float('inf')
        patience = 0
        
        for epoch in range(1, Config.EPOCHS + 1):
            print(f"\n--- Epoch {epoch}/{Config.EPOCHS} (LR: {scheduler.get_last_lr()[0]:.2e}) ---")
            
            train_loss = train_epoch(model, train_loader, criterion, optimizer,
                                     scaler, Config, Config.ACCUMULATION_STEPS)
            
            scheduler.step()
            
            overall, mae, count_mae, cont_mae = validate(model, val_loader, 
                                                          normalizer, Config)
            
            improved = False
            if count_mae < best_count_mae:
                best_count_mae = count_mae
                best_overall_mae = overall
                patience = 0
                improved = True
                
                torch.save({
                    'model': model.state_dict(),
                    'normalizer': normalizer.save(),
                    'best_count_mae': best_count_mae,
                    'best_overall_mae': best_overall_mae,
                    'epoch': epoch,
                    'config': {
                        'backbone': Config.BACKBONE,
                        'img_size': Config.IMG_SIZE,
                        'targets': Config.TARGET_COLS,
                    }
                }, Config.OUTPUT_DIR / f"best_fold{fold}.pt")
                print(f"üèÜ NEW BEST! Count MAE: {count_mae:.2f}, Overall: {overall:.2f}")
            else:
                patience += 1
            
            print(f"Loss: {train_loss:.4f} | Count MAE: {count_mae:.2f} | Overall: {overall:.2f}")
            print_metrics(mae, Config.TARGET_COLS, count_mae, cont_mae)
            
            if patience >= 15:
                print("Early stopping")
                break
        
        results.append({
            'fold': fold, 
            'best_count_mae': best_count_mae,
            'best_overall_mae': best_overall_mae
        })
        print(f"\n‚úÖ Fold {fold} Best: Count MAE={best_count_mae:.2f}, Overall={best_overall_mae:.2f}")
    
    print("\n" + "=" * 70)
    print("üèÜ TRAINING COMPLETE")
    print("=" * 70)
    for r in results:
        print(f"Fold {r['fold']}: Count MAE={r['best_count_mae']:.2f}, Overall={r['best_overall_mae']:.2f}")
    
    avg_count = np.mean([r['best_count_mae'] for r in results])
    avg_overall = np.mean([r['best_overall_mae'] for r in results])
    print(f"\nAverage: Count MAE={avg_count:.2f}, Overall={avg_overall:.2f}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--data-dir', default='RiceData/Unido_AfricaRice_Challenge')
    parser.add_argument('--output-dir', default='outputs/v6_optimized')
    parser.add_argument('--model', default='convnext_xlarge.fb_in22k')
    parser.add_argument('--img-size', type=int, default=512)
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--batch-size', type=int, default=4)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--count-weight', type=float, default=5.0)
    parser.add_argument('--folds', default='0')
    args = parser.parse_args()
    
    Config.DATA_DIR = Path(args.data_dir)
    Config.IMAGE_DIR = Config.DATA_DIR / "unido_rice_images"
    Config.OUTPUT_DIR = Path(args.output_dir)
    Config.BACKBONE = args.model
    Config.IMG_SIZE = args.img_size
    Config.EPOCHS = args.epochs
    Config.BATCH_SIZE = args.batch_size
    Config.LEARNING_RATE = args.lr
    Config.COUNT_WEIGHT = args.count_weight
    Config.FOLD_TO_RUN = [int(f) for f in args.folds.split(',')]
    
    main()
